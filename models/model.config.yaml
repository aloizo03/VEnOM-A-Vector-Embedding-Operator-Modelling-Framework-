num_layers : 16
d_token: 40
head_num: 4
attention_hidden_dim: 1024
attention_type_of_layer: "Pre-LN"
attention_dropout: 0.2
ffn_dropout: 0.3
ffn_dim: 1024
d_out: 10
activation: "relu"
weight_decay: 0.1
momentum: 0.8
lrf: 0.1
mlp_hidden_dim: 64

#num_layers : 1
#d_token: 40
#head_num: 1
#attention_hidden_dim: 64
#attention_type_of_layer: "Pre-LN"
#attention_dropout: 0.2
#ffn_dropout: 0.2
#ffn_dim: 128
#d_out: 10
#activation: "relu"
#weight_decay: 0.3
#momentum: 0.8
#lrf: 0.1
#mlp_hidden_dim: 64
