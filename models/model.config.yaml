num_layers : 16
input_dim: 20
d_token: 300
head_num: 5
attention_hidden_dim: 1024
attention_type_of_layer: "Pre-LN"
attention_dropout: 0.1
ffn_dropout: 0.4
ffn_dim: 1024
d_out: 10
activation: "relu"
weight_decay: 0.1
momentum: 0.8
lrf: 0.1
mlp_hidden_dim: 64

read_all_dataset: True
table_train: True
dim_k: 100